{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e82860d-2592-4be2-bbf2-7124964d6df8",
   "metadata": {
    "id": "6e82860d-2592-4be2-bbf2-7124964d6df8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.transformer import TLM\n",
    "from model.utils import n_params\n",
    "from tokens import Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c9d7f1-b469-4b91-b864-2790e04dd979",
   "metadata": {
    "id": "f0c9d7f1-b469-4b91-b864-2790e04dd979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "d_opts = [('cuda', torch.cuda.is_available()), ('mps', torch.backends.mps.is_available()), ('cpu', True)]\n",
    "device = next(device for device, available in d_opts if available)\n",
    "print(f'using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c948d1f7-0c44-4bd5-b865-9cd2c9bf537b",
   "metadata": {
    "id": "c948d1f7-0c44-4bd5-b865-9cd2c9bf537b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/1000\r"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "with open ('data/truths.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read() # 15,057 unique words\n",
    "    tks = Tokens(corpus, vocab_size) # 50,000 in gpt-2\n",
    "tokenized = tks.tokenize(corpus)\n",
    "encoded = tks.encode(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "41eb3d26-3457-4312-b805-a29d49a07100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (129088 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "with open ('data/truths.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read() # 15,057 unique words\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "tokenized = tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "WZqFf4IvoNrR",
   "metadata": {
    "id": "WZqFf4IvoNrR"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(tokenized['input_ids'], dtype=torch.long, device=device)\n",
    "n = int(0.85*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d412055f-3cf4-433c-89b1-6291a7974037",
   "metadata": {
    "id": "d412055f-3cf4-433c-89b1-6291a7974037"
   },
   "outputs": [],
   "source": [
    "def get_batch(split: str):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26fb22c4-65dc-4d59-873a-9d990c8d2893",
   "metadata": {
    "id": "26fb22c4-65dc-4d59-873a-9d990c8d2893"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m, eval_iters: int=10):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "68ee8709-9236-440a-b8db-10dee2c6c44d",
   "metadata": {
    "id": "68ee8709-9236-440a-b8db-10dee2c6c44d"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 128 # 1024 in gpt2\n",
    "n_embd = 96 # 768 in gpt2\n",
    "n_blocks = 8 # 24 in gpt2\n",
    "n_heads = 4\n",
    "\n",
    "lr = 1e-3\n",
    "iters = 250\n",
    "i_eval = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffc4fbef-78fc-4b8a-a86f-8a182435634c",
   "metadata": {
    "id": "ffc4fbef-78fc-4b8a-a86f-8a182435634c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of params: 10604305\n"
     ]
    }
   ],
   "source": [
    "model = TLM(block_size=block_size, n_embd=n_embd, vocab_size=vocab_size,\n",
    "            n_blocks=n_blocks, n_heads=n_heads, device=device).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(f'num of params: {n_params(model)}') # gpt-2 has 1,500,000,000 (1.5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "581b8b15-8f7a-4751-b1d1-07263d340950",
   "metadata": {
    "id": "581b8b15-8f7a-4751-b1d1-07263d340950"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [01:44<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training took: 105.00s or 1.75m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "model.train()\n",
    "for i in tqdm(range(iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #lr = 1e-4 if i > 250 else 1e-2\n",
    "    #if i % i_eval == 0:\n",
    "        #tv_loss = estimate_loss(model)\n",
    "        #print(f\"step {i}: train loss {tv_loss['train']:.4f} val loss {tv_loss['val']:.4f}\")\n",
    "et = time.time()\n",
    "print()\n",
    "print(f'training took: {et-st:.2f}s or {(et-st)/60:.2f}m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "72fe501f-cbc0-4616-b753-a47d01d70fb8",
   "metadata": {
    "id": "72fe501f-cbc0-4616-b753-a47d01d70fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- After Training\n",
      "train loss: 3.5151 val loss: 4.6796\n"
     ]
    }
   ],
   "source": [
    "print('-- After Training')\n",
    "tv_loss = estimate_loss(model)\n",
    "print(f\"train loss: {tv_loss['train']:.4f} val loss: {tv_loss['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cf5aab9e-4dfd-425b-8a59-e610f3417f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "a dinosaur from a kind of 78 harder\n",
      "end into many\n",
      "a new species no near-like of substance\n",
      "s temperature is a candle\n",
      "star is a kind of substance contains fe\n",
      "a runoff is a kind of whole into 0 walls substance\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=50).tolist()[0]\n",
    "print(tokenizer.decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3aa4169d-b871-49cb-9e00-b137015167d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice), max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dout \u001b[38;5;241m=\u001b[39m \u001b[43mtks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tks\u001b[38;5;241m.\u001b[39mdetokenize(dout))\n",
      "File \u001b[0;32m~/Dev/txtgen/tokens.py:37\u001b[0m, in \u001b[0;36mTokens.__init__.<locals>.<lambda>\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitos \u001b[38;5;241m=\u001b[39m {i:p \u001b[38;5;28;01mfor\u001b[39;00m i,p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)}\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m s]\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m e: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m e])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m ts: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(t\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mĠ\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mĊ\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m})) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ts)\n",
      "\u001b[0;31mKeyError\u001b[0m: 2033"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=50)\n",
    "dout = tks.decode(out[0].tolist())\n",
    "print(tks.detokenize(dout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25815985-0418-49e8-a06f-2f78b8e441bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "notebook_venv",
   "language": "python",
   "name": "notebook_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
